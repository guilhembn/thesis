\ifdefined\included
\else
\documentclass[a4paper,11pt,twoside]{StyleThese}
\include{formatAndDefs}
\sloppy
\begin{document}
\fi


\chapter*{Conclusion}
\addstarredchapter{Conclusion} %Sinon cela n'apparait pas dans la table des mati√®res
\markboth{CONCLUSION}{}

In this thesis we presented several contributions focusing on endowing the robot with ability to plan explicity for itslef and for the human. Indeed, several approaches to human-aware navigation planning (and motion planning in general) account for human presence by including social costs influencing the trajectory, but only plan a course of actions for the robot. While being successful when humans are static or when evolving in large environment, these approaches are challenged when the interaction becomes intricate, where the robot and the human must collaborate to solve a problem. In robotic task planning, where human robot intricate interactions are easier to explore, some approaches do include planning for both the human and the robot. However, these approaches seldom maintain different beliefs for both agents during the planning process, whereas it is what a human is expecting according to joint action theory. Moreover, these approaches consider the human as a totally controllable agent, close to what is done in multi robot planning. They do not account for communications needed to align beliefs or to share the plan.

In Chapter~\ref{chapter:navigation}, we showed why planning for both the human and the robot is important for navigation planning in intricate interaction scenarios. Not only it allows to find valid solutions where other approaches would have not, but, by anticipating the possible human trajectory we can make the robot more efficient and its behavior more satisfactory for the human.  Indeed, by estimating the human future positions and speed, we are able to make the robot trajectory less threatening, more legible and to enhance the mutual manifestness of the robot. These results were validated through a user study, involving a totally autonomous PR2 robot crossing a human in a narrow corridor. We also presented how this navigation scheme has been implemented on other robots, including a HRP2 humanoid robot and a Pepper robot which has been deployed autonomously for several weeks providing route description to customers in a mall.

Alleviating from ephemeral nature of human robot navigation interaction, we presented a new way of planning for communication during task planning in Chapter~\ref{chapter:comm}. We chose to make a hybrid planning approach where a domain-independent HTN planner (HATP) delegates the resolution of feasibility and cost of communication actions to a domain-specific communication planner. We focused on communication actions needing to designate objects to the other agent. Thus, we needed a domain-specific planner able to determine the content of such communications. This problem is called referring expression generation and, albeit studied for a long time, we did not find any suitable existing work to be integrated in a human robot task planning scheme. Indeed, to be used in task planning, such a planner must be efficient and some specific constraints imposed by human robot interaction have to be satisfied. We formalized the REG problem for HRI using an ontology as a knowledge base and we proposed an efficient algorithm to solve it. This algorithm has been shown to be the most efficient to our knowledge while being designed for HRI scenarios. It then has been integrated in HATP, an HTN planner able to maintain beliefs of multiple agents during task planning. Resolving the content of communication actions at this stage is only doable if the planner plans for both human and robot, it allows to prevent to reach situations where the execution is blocked because a communication action cannot be performed and also improve the quality of plans.

However, HATP relies on exploring only one hierarchical task network (HTN) and allocate task to either the robot or the human depending on the task constraints to find an optimal plan. Representing interactive tasks this way leads to execution where the human is considered as knowing the plan before it begins. Indeed, several works use similar approaches and solve contingencies (\textit{e.g.} beliefs divergence, plan communication) during the plan execution. 

In Chapter~\ref{chapter:doublehtn} we propose a new task planning scheme enriching current human-aware task planners. The general idea is to not only keep distinct beliefs between the robot and the human but also have two separated action models. Both models are HTNs, but do not represent the same concepts. The robot HTN, as in classical HTN planning, is designed to give expert knowledge to the planner on the different ways for the robot to perform a task. The human HTN on the other hand, is closer to a human task model as used in interactive systems engineering in human computer interaction. It aims at representing how a human may achieve a task (\textit{i.e.} emulating parts of their planning process) and how they may react to a particular world state or to a robot request. The presented planner uses both HTNs to elaborate valid conditional plans then selects the optimal one. These conditional plans contain the possible human actions deduced via their task model. We presented our approach in scenarios involving intricate human robot interactions and showed how it is suitable and results in interesting plans. 

Finally, in Chapter~\ref{chapter:integration}, we presented how this new task planning scheme can be integrated into a complete robotic architecture dedicated to HRI. Moreover, we introduced a new task for HRI inspired from psychology experiments along with an implemented robotic architecture including our planning scheme, allowing to tackle some of the challenges of this task.

\section*{On the human agent interaction guidelines and joint action theory principles}
\markright{HUMAN-AGENT INTERACTION GUIDELINES}
We presented in Chapter~\ref{chapter:sota} maxims coined by Bradshaw \textit{et al.}~\cite{bradshaw2011human} for human-agent interaction. We propose here to sum up which maxims have guided the work presented in this thesis and to what extent we were able to implement them.

By completely exploring the search space with the proposed planning method of Chapter~\ref{chapter:doublehtn}, we can increase the \textit{progress appraisal} of the robot. Indeed, by returning all the conditional plan to the supervision it can compute how much the task is progressing, and communicate it to the human if needed. Besides, we proposed a plan post-processing step where robot actions are added to guide human actions away from potential errors and to the optimal plan.

Moreover, by explicitly representing the action effects in the beliefs of both agent, we explicitly represent the \textit{observability} of the robot. Moreover, as we consider some effects (or even some actions) to be not observable, and thus, not updating the human beliefs, the robot observability will impact the elaborated plan.

Then, we showed our approach can be used to balance plans where the robot is proactive and ones where it let the human choose the tasks attributions. This matches to the agent \textit{knowing its limits} maxim. Moreover, unlike HATP, the human decisions are not set, the planner provide a robot course of action for multiple possible human actions, thus adapting to the human choices. We also envision to use the HTNs exploration to guide the human choices or inform them about potential outcomes.

Moreover, by representing robot unknown human beliefs, the robot is able to plan to ask question and predict possible human answers to them, making the robot more \textit{directable}. A strong link between the task planning process and the supervision is required to explore further this maxim.

Similarly, a first step has been made towards the negotiation and deconfliction of plans and beliefs alignment, increasing the robot \textit{coordination}. Some possible approaches where presented in Chapter~\ref{chapter:doublehtn} to increase it even more by interacting with the human during the plan elaboration process, in addition to the human planing process emulation. This also requires a stronger link with supervision, currently being explored.

Besides, we allowed to make the robot more \textit{selective} as we showed in Chapter~\ref{chapter:doublehtn}, by allowing to plan belief alignment actions only when they are needed, and only with the beliefs required by the human to perform better.

Finally, we showed in Chapter~\ref{chapter:navigation} that planning navigation for both the human and the robot allows to make the robot deconflict the trajectories earlier increasing the robot \textit{predictability}. Moreover, we implemented a head behavior for the navigation aiming at showing its future trajectory while acknowledging the human presence.

\section*{Limitations and future work}
\markright{LIMITATIONS AND FUTURE WORK}
We think there is plenty of potential for the task planning approach presented in Chapter~\ref{chapter:doublehtn} and it must be refined and enriched. While it seems promising, as it allows to represent and find plans for intricate interaction scenarios, has been integrated with a domain-specific communication planner and used into a robotic architecture, limitations can be identified. First, the turn-taking approach used does not translate the duration of actions. Indeed, some actions will be longer than others, and not accounting for it may lead to suboptimal plans and wrong prediction of human actions. Then, by not pruning some part of the exploration graph during the search, the approach is not efficient for large HTN domains. Efficiency can also by gained by a better implementation in C++ rather than in Python. However, pruning while searching would prevent applying plan-wide costs.


\subsection*{Bringing planning and supervision closer}
Some future work has already been identified to extend the approaches presented in this thesis. The first one, already mentioned before, is to build a stronger link with supervision. In common robotic architectures, the link between the supervision and geometric and task planning come down to planning request and plan response. However, this link may not be enough for long term interaction or intricate and dynamic situations usually found in HRI.

For HATEB, the navigation scheme presented in Chapter~\ref{chapter:navigation}, the solution proposed assumes the human will respect the model provided, and adjust the robot trajectory accordingly. For example, if the robot is following the human in a narrow corridor, if the human goes slower than what is set in the navigation planner parameters, the robot will never overtake them. Indeed, the planned trajectory for the human is for them to accelerate making the robot following the human permanently. The human model provided to the planner ($\humanmodel$) must be as accurate as possible. Providing a perfect model for each human encountered is obviously impossible, that is why the supervision must not only provide an initial model to the planner with the planning request but must also update this human model during the execution, especially if contingencies are detected to happen while following the plan. Here for example, a supervision system might decrease the human speed parameter if they are repeatedly detected to move slower than expected.

For the REG algorithm presented in Chapter~\ref{chapter:comm} this human model update is also crucial to have a good estimate of communication capabilities of the human and the associated difficulties to understand. As presented, some relations to describe an object are more difficult to understand than others. In our approach, we represented it by a cost associated to each properties. This difficulty depends on the person the robot is interacting with. For example, using color to refer to an object is less efficient or even impossible when speaking to a color blind person. Our costs are indeed defined per human we are interacting with. Besides, the difficulty to understand is also context dependent. For instance, colors relation can be hard or even impossible to perceive if the scene is lit with colored lights. Again, to cope with these issue, the supervision must update on the human model used for planning during the execution. Moreover, it can request and iterate with the planner during the planning process to allow for more or less risky communications, leading to more or less efficient plans.

The same is true for the planning approach depicted in Chapter~\ref{chapter:doublehtn}. The human action model along with associated costs must be updated on a per-human basis all along the interaction. By refining the human model the best prediction would be made for them, leading to more efficient plans. Besides, some task or actions can be enabled or disabled depending on the human and their level of expertise in the task and for robot collaboration. Heuristics can also be learned as to which decomposition a human may use for a task in a specific context. Highly probable decompositions can then be explored first, resulting in a more efficient planning process.

To reduce the branching factor in human HTN exploration, we can also try to negotiate the plan while elaborating it. For example, if too many human actions are returned during the search, the planning process can request the supervision to propose the different task alternatives to the human and ask which one they would perform in the specific state the planner is in. Only the answered alternatives can then be explored by the planner. Not only it would help reducing the branching factor, but the robot may appear more \textit{predictable} and the plan more \textit{explainable} as some actions would have been chosen by the human. Thanks to the HTN structure, communication about the tasks made easier. This negotiation may need multiple iterations as the alternatives proposed by the human may lead to unfeasible plans. However, if the choice is proposed for a point too far in the future, the human may have a hard time projecting themselves in that situation.

In addition to considering the possible human actions in the conditional plan, the planner could also expose the effects of the actions and more precisely the observable effects of them. By doing so, the supervision would know what to expect from the human and what to monitor to determine which action the human did, influencing the branch of the plan executed. Going further, the supervision may use the entire human action model to be able to also predict human behavior, in case of plan repair for example.

\subsection*{Theory of mind level up}
To make a better prediction of the human decisions and actions, some advanced scenarios require the robot to represent the model the human has made of it ($\robotinhumanmodel$). Indeed, as shown by Chakraborti \textit{et al.}~\cite{chakraborti2017plan}, using it can lead to more legible and predictable plans. As described in the Chapter~\ref{chapter:sota}, joint action theory informs about the capabilities a human is using when interacting with another agent. Especially, humans can \textit{predict} other's actions and \textit{integrate them into their own plan}. Thus, the model the human is making about the robot will influence their decision process and how they will perform a task. This is why it is important to build this model. One approach to do so is to analyze the actions performed by the robot by taking the perspective of the human and emulate an inferring process to build a robot model.

For example, in the navigation scheme from Chapter~\ref{chapter:navigation}, integrating this model would lead to a better prediction of human trajectory. The robot trajectory costs would also be more accurate, as more constraints could be added. A surprise cost for instance can be estimated by comparing the planned robot trajectory ($\robotmodel$), with the one expected by the human ($\robotinhumanmodel$). This could, in turn, be used to better respect and evaluate the respect of the maxim of \textit{predictability} and \textit{dependability}.

Likewise, in the planning approach presented in Chapter~\ref{chapter:doublehtn}, using the estimation of the robot model that the human has would result in more accurate predictions of human actions. Indeed, we know that the human will integrate actions of other agents in their own plan, as showed by joint action theory. Until now, we assumed that it would not be the case as we envisioned interaction with novice users who have never interacted with a robot before, or not enough to be confident to integrate the robot action in their planning process. But, as they would gain experience, trust and habits, human partners will expect the robot to perform in a certain way. Plans quality would increase by integrating these expectations in the emulation of human planning process. Moreover, we could improve the \textit{directability} and \textit{observability} of the robot by adding possible actions explicitly updating the human's robot model.


\ifdefined\included
\else
\bibliographystyle{acm}
\bibliography{These}
\end{document}
\fi